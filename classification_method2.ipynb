{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import autograd as ag\n",
    "from mxnet import gluon\n",
    "from mxnet import init\n",
    "from mxnet import nd\n",
    "from time import time\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.model_zoo import vision as models\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "project_path = '/home/carsmart/users/xiaoming/Distracted_Driver_Detection/'\n",
    "path_list = project_path + 'train_lst/train.lst'\n",
    "path_rec = project_path + 'train_lst/train.rec'\n",
    "val_path_list = project_path + 'train_lst/val_val.lst'\n",
    "val_path_rec = project_path + 'train_lst/val_val.rec'\n",
    "test_path_list = project_path + 'test_lst/test_test.lst'\n",
    "test_path_rec = project_path + 'test_lst/test_test.rec'\n",
    "\n",
    "save_path = project_path + 'model/'\n",
    "\n",
    "(mean_r, mean_g, mean_b) = (0.485 * 255, 0.456 * 255, 0.406 * 255)\n",
    "(std_r, std_g, std_b) = (0.229 * 225, 0.224 * 255, 0.225 * 255)\n",
    "\n",
    "batch_size = 16\n",
    "ctx = [mx.gpu(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_iter(kv, resize, data_shape):\n",
    "    train_iter = mx.io.ImageRecordIter(\n",
    "        path_imglist = path_list, \n",
    "        path_imgrec = path_rec, \n",
    "        resize = resize, \n",
    "        data_shape = data_shape, \n",
    "        batch_size = batch_size, \n",
    "        rand_mirror = False, \n",
    "        rand_crop = False, \n",
    "        mean_r = mean_r, \n",
    "        mean_g = mean_g, \n",
    "        mean_b = mean_b, \n",
    "        std_r = std_r, \n",
    "        std_g = std_g, \n",
    "        std_b = std_b, \n",
    "        num_parts = kv.num_workers, \n",
    "        part_index = kv.rank, \n",
    "        shuffle = True\n",
    "    )\n",
    "    train_iter = mx.io.PrefetchingIter(train_iter)\n",
    "    \n",
    "    val_iter = mx.io.ImageRecordIter(\n",
    "        path_imglist = val_path_list, \n",
    "        path_imgrec = val_path_rec, \n",
    "        resize = resize, \n",
    "        data_shape = data_shape, \n",
    "        batch_size = batch_size, \n",
    "        rand_mirror = False, \n",
    "        rand_crop = False, \n",
    "        mean_r = mean_r, \n",
    "        mean_g = mean_g, \n",
    "        mean_b = mean_b, \n",
    "        std_r = std_r, \n",
    "        std_g = std_g, \n",
    "        std_b = std_b, \n",
    "        num_parts = kv.num_workers, \n",
    "        part_index = kv.rank\n",
    "    )\n",
    "    \n",
    "    test_iter = mx.io.ImageRecordIter(\n",
    "        path_imglist = test_path_list, \n",
    "        path_imgrec = test_path_rec, \n",
    "        resize = resize, \n",
    "        data_shape = data_shape, \n",
    "        batch_size = batch_size, \n",
    "        rand_mirror = False, \n",
    "        rand_crop = False, \n",
    "        mean_r = mean_r, \n",
    "        mean_g = mean_g, \n",
    "        mean_b = mean_b, \n",
    "        std_r = std_r, \n",
    "        std_g = std_g, \n",
    "        std_b = std_b, \n",
    "        num_parts = kv.num_workers, \n",
    "        part_index = kv.rank\n",
    "    )\n",
    "    \n",
    "    return(train_iter, val_iter, test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model define\n",
    "use model define in mxnet.gluon.model_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pretrained_net = models.get_model(name = 'densenet161', pretrained = True, ctx = ctx, prefix = 'densenet161_')\n",
    "net = models.get_model(name = 'densenet161', classes = 10, prefix = 'densenet161_')\n",
    "net.features = pretrained_net.features\n",
    "net.output.initialize(init.Xavier(), ctx = ctx)\n",
    "net.hybridize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(batch, ctx):\n",
    "    \"\"\"return data and label on ctx\"\"\"\n",
    "    if isinstance(batch, mx.io.DataBatch):\n",
    "        data = batch.data[0]\n",
    "        label = batch.label[0]\n",
    "    else:\n",
    "        data, lable = batch\n",
    "    return (gluon.utils.split_and_load(data, ctx), \n",
    "           gluon.utils.split_and_load(label, ctx), \n",
    "           data.shape[0])\n",
    "\n",
    "def evaluate_accuracy(data_iterator, net, ctx = [mx.cpu()]):\n",
    "    if isinstance(ctx, mx.Context):\n",
    "        ctx = [ctx]\n",
    "    acc = nd.array([0])\n",
    "    losses = nd.array([0])\n",
    "    n = 0.\n",
    "    if isinstance(data_iterator, mx.io.MXDataIter):\n",
    "        data_iterator.reset()\n",
    "    for batch in data_iterator:\n",
    "        data, label, batch_size = get_batch(batch, ctx)\n",
    "        for x, y in zip(data, label):\n",
    "            outputs = net(x)\n",
    "            acc += nd.sum(outputs.argmax(axis = 1) == y).copyto(mx.cpu())\n",
    "            losses += nd.sum(loss(outputs, y)).copyto(mx.cpu())\n",
    "            n += y.size\n",
    "        nd.waitall() # don't push too many operators into backend\n",
    "    return acc.asscalar() / n, losses.asscalar() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(kv, net, loss, trainer, ctx, num_epochs, \n",
    "          print_batches = None, save_epochs = 10000):\n",
    "    \"\"\"train a net work\"\"\"\n",
    "    print(\"Start training on \", ctx)\n",
    "    if isinstance(ctx, mx.Context):\n",
    "        ctx = [ctx]\n",
    "    for epoch in range(num_epochs):\n",
    "        (train_data, val_data, _) = get_iter(kv, resize = resize, data_shape = data_shape)\n",
    "        train_loss, train_acc, n, m = 0.0, 0.0, 0.0, 0.0\n",
    "        if isinstance(train_data, (mx.io.PrefetchingIter, mx.io.MXDataIter)):\n",
    "            train_data.reset()\n",
    "        start = time()\n",
    "        for i, batch in enumerate(train_data):\n",
    "            data, label, batch = get_batch(batch, ctx)\n",
    "            losses = []\n",
    "            with ag.record():\n",
    "                outputs = [net(x) for x in data]\n",
    "                losses = [loss(yhat, y) for yhat, y in zip(outputs, label)]\n",
    "            for l in losses:\n",
    "                l.backward()\n",
    "            train_acc += sum([(yhat.argmax(axis = 1) == y).sum().asscalar() \n",
    "                             for yhat, y in zip(outputs, label)])\n",
    "            train_loss += sum([l.sum().asscalar() for l in losses])\n",
    "            trainer.step(batch_size)\n",
    "            n += batch_size\n",
    "            m += sum([y.size for y in label])\n",
    "            if print_batches and (i + 1) % print_batches == 0:\n",
    "                print(\"Data %d. Loss: %f, Train acc %f, Time %.1f sec\" % (\n",
    "                    n, train_loss / n, train_acc / m, time() - start\n",
    "                ))\n",
    "        test_acc, test_loss = evaluate_accuracy(val_data, net, ctx)\n",
    "        # test_acc = 0\n",
    "        # if not epoch % save_epochs:\n",
    "        #     net.collect_params().save(save_path + 'method2' + str(epoch) + '00.params')\n",
    "        print(\"Epoch %d. Loss: %.3f, Train acc %.2f, Test acc %.2f, Test loss %.3f, Time %.1f sec, learning_rate %f\" %(\n",
    "            epoch, train_loss / n, train_acc / m, test_acc, test_loss, time() - start, trainer.learning_rate\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training on  [gpu(1)]\n",
      "Data 16000. Loss: 0.148729, Train acc 0.962625, Time 276.3 sec\n",
      "Epoch 0. Loss: 0.135, Train acc 0.97, Test acc 0.81, Test loss 0.602, Time 333.5 sec, learning_rate 0.000050\n",
      "Data 16000. Loss: 0.007701, Train acc 0.998875, Time 277.2 sec\n",
      "Epoch 1. Loss: 0.007, Train acc 1.00, Test acc 0.82, Test loss 0.525, Time 334.0 sec, learning_rate 0.000050\n",
      "Data 16000. Loss: 0.008527, Train acc 0.998000, Time 277.4 sec\n",
      "Epoch 2. Loss: 0.010, Train acc 1.00, Test acc 0.84, Test loss 0.498, Time 334.2 sec, learning_rate 0.000050\n",
      "Data 16000. Loss: 0.009712, Train acc 0.998062, Time 277.0 sec\n",
      "Epoch 3. Loss: 0.011, Train acc 1.00, Test acc 0.86, Test loss 0.453, Time 333.6 sec, learning_rate 0.000050\n",
      "Data 16000. Loss: 0.010765, Train acc 0.997500, Time 277.3 sec\n",
      "Epoch 4. Loss: 0.010, Train acc 1.00, Test acc 0.85, Test loss 0.463, Time 334.6 sec, learning_rate 0.000050\n",
      "Data 16000. Loss: 0.008779, Train acc 0.997250, Time 278.2 sec\n",
      "Epoch 5. Loss: 0.008, Train acc 1.00, Test acc 0.89, Test loss 0.358, Time 335.3 sec, learning_rate 0.000050\n",
      "Data 16000. Loss: 0.000907, Train acc 0.999812, Time 277.4 sec\n",
      "Epoch 6. Loss: 0.001, Train acc 1.00, Test acc 0.89, Test loss 0.387, Time 334.3 sec, learning_rate 0.000050\n",
      "Data 16000. Loss: 0.000102, Train acc 1.000000, Time 278.4 sec\n",
      "Epoch 7. Loss: 0.000, Train acc 1.00, Test acc 0.89, Test loss 0.382, Time 335.6 sec, learning_rate 0.000050\n",
      "Data 16000. Loss: 0.000057, Train acc 1.000000, Time 279.2 sec\n",
      "Epoch 8. Loss: 0.000, Train acc 1.00, Test acc 0.90, Test loss 0.380, Time 336.5 sec, learning_rate 0.000050\n",
      "Data 16000. Loss: 0.000035, Train acc 1.000000, Time 278.6 sec\n",
      "Epoch 9. Loss: 0.000, Train acc 1.00, Test acc 0.90, Test loss 0.378, Time 336.2 sec, learning_rate 0.000050\n"
     ]
    }
   ],
   "source": [
    "kv = mx.kvstore.create(\"local\")\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "learning_rate = 5e-5\n",
    "wd = 1e-6\n",
    "lr_step = 400\n",
    "lr_factor = 0.9\n",
    "resize = 224\n",
    "data_shape = (3, 224, 224)\n",
    "\n",
    "num_epochs = 10\n",
    "lr_scheduler = mx.lr_scheduler.FactorScheduler(step = lr_step, factor = lr_factor)\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), \n",
    "                       'adam', {'learning_rate': learning_rate})\n",
    "train(kv, net, loss, trainer, ctx, num_epochs = num_epochs, \n",
    "      print_batches = 1000, save_epochs = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.collect_params().save(save_path + 'method2_' + '02.params')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _get_batch(batch, ctx):\n",
    "    \"\"\"return data and label on ctx\"\"\"\n",
    "    if isinstance(batch, mx.io.DataBatch):\n",
    "        data = batch.data[0]\n",
    "        label = batch.label[0]\n",
    "    else:\n",
    "        data, lable = batch\n",
    "    return (gluon.utils.split_and_load(data, ctx), \n",
    "           gluon.utils.split_and_load(label, ctx), \n",
    "           data.shape[0])\n",
    "\n",
    "def net_predict(net, data_iter, ctx = [mx.cpu()]):\n",
    "    outputs = []\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        real_size = batch_size - batch.pad\n",
    "        data, _, _ = _get_batch(batch, ctx)\n",
    "        output = [net(x) for x in data]\n",
    "        outputs.append(nd.concatenate([x.as_in_context(mx.cpu()) for x in output])[0:real_size, :])\n",
    "        nd.waitall()\n",
    "        \n",
    "    outputs = nd.concatenate(outputs)\n",
    "    return(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(_, _, test_iter) = get_iter(kv, resize = resize, data_shape = data_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = net_predict(net, test_iter, ctx = ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = nd.softmax(output).asnumpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## match submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_save_path = project_path + 'predict/'\n",
    "import pandas as pd\n",
    "df_pred = pd.read_csv(project_path + 'sample_submission.csv')\n",
    "test_pictures = pd.read_table(test_path_list, header = None)[2]\n",
    "df_pred['img'] = test_pictures\n",
    "for i, j in enumerate(df_pred.columns[1:]):\n",
    "    df_pred[j] = output[:, i]\n",
    "\n",
    "df_pred = df_pred.sort_values('img')\n",
    "df_pred.to_csv(predict_save_path + 'pred6.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
